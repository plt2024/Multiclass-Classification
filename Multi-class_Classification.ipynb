{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multi-class Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn the different strategies of Multi-class classification and implement the same on a real-world dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Objectives**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the use of one-hot encoding for categorical variables.\n",
    "2. Implement logistic regression for multi-class classification using **One-vs-All (OvA)** and **One-vs-One (OvO)** strategies.\n",
    "3. Evaluate model performance using appropriate metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to ensure the availability of the required libraries, execute the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.2.0\n",
      "  Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.0\n",
      "Collecting pandas==2.2.3\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas==2.2.3) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas==2.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas==2.2.3) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.3)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m152.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.1\n",
      "Collecting scikit-learn==1.6.0\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.6.0) (2.2.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.6.0)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.6.0)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.6.0)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "Collecting matplotlib==3.9.3\n",
      "  Downloading matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.9.3)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.9.3)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.9.3)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.9.3)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.3) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.3) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib==3.9.3)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.9.3)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.3) (1.17.0)\n",
      "Downloading matplotlib-3.9.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.9.3 pillow-11.1.0 pyparsing-3.2.1\n",
      "Collecting seaborn==0.13.2\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.12/site-packages (from seaborn==0.13.2) (2.2.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.12/site-packages (from seaborn==0.13.2) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.12/site-packages (from seaborn==0.13.2) (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn==0.13.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn==0.13.2) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn==0.13.2) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==2.2.0\n",
    "!pip install pandas==2.2.3\n",
    "!pip install scikit-learn==1.6.0\n",
    "!pip install matplotlib==3.9.3\n",
    "!pip install seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import the necessary libraries for data processing, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "The data set being used for this lab is the \"Obesity Risk Prediction\" data set publically available on <a href=\"https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition\">UCI Library</a> under the <a href=\"https://creativecommons.org/licenses/by/4.0/legalcode\">CCA 4.0</a> license. The data set has 17 attributes in total along with 2,111 samples. \n",
    "\n",
    "The attributes of the dataset are descibed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-7zrl{text-align:left;vertical-align:bottom}\n",
    "</style>\n",
    "<table class=\"tg\"><thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-7zrl\">Variable Name</th>\n",
    "    <th class=\"tg-7zrl\">Type</th>\n",
    "    <th class=\"tg-7zrl\">Description</th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">Gender</td>\n",
    "    <td class=\"tg-7zrl\">Categorical</td>\n",
    "    <td class=\"tg-7zrl\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">Age</td>\n",
    "    <td class=\"tg-7zrl\">Continuous</td>\n",
    "    <td class=\"tg-7zrl\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">Height</td>\n",
    "    <td class=\"tg-7zrl\">Continuous</td>\n",
    "    <td class=\"tg-7zrl\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">Weight</td>\n",
    "    <td class=\"tg-7zrl\">Continuous</td>\n",
    "    <td class=\"tg-7zrl\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">family_history_with_overweight</td>\n",
    "    <td class=\"tg-7zrl\">Binary</td>\n",
    "    <td class=\"tg-7zrl\">Has a family member suffered or suffers from overweight?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">FAVC</td>\n",
    "    <td class=\"tg-7zrl\">Binary</td>\n",
    "    <td class=\"tg-7zrl\">Do you eat high caloric food frequently?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">FCVC</td>\n",
    "    <td class=\"tg-7zrl\">Integer</td>\n",
    "    <td class=\"tg-7zrl\">Do you usually eat vegetables in your meals?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">NCP</td>\n",
    "    <td class=\"tg-7zrl\">Continuous</td>\n",
    "    <td class=\"tg-7zrl\">How many main meals do you have daily?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">CAEC</td>\n",
    "    <td class=\"tg-7zrl\">Categorical</td>\n",
    "    <td class=\"tg-7zrl\">Do you eat any food between meals?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">SMOKE</td>\n",
    "    <td class=\"tg-7zrl\">Binary</td>\n",
    "    <td class=\"tg-7zrl\">Do you smoke?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">CH2O</td>\n",
    "    <td class=\"tg-7zrl\">Continuous</td>\n",
    "    <td class=\"tg-7zrl\">How much water do you drink daily?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">SCC</td>\n",
    "    <td class=\"tg-7zrl\">Binary</td>\n",
    "    <td class=\"tg-7zrl\">Do you monitor the calories you eat daily?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">FAF</td>\n",
    "    <td class=\"tg-7zrl\">Continuous</td>\n",
    "    <td class=\"tg-7zrl\">How often do you have physical activity?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">TUE</td>\n",
    "    <td class=\"tg-7zrl\">Integer</td>\n",
    "    <td class=\"tg-7zrl\">How much time do you use technological devices such as cell phone, videogames, television, computer and others?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">CALC</td>\n",
    "    <td class=\"tg-7zrl\">Categorical</td>\n",
    "    <td class=\"tg-7zrl\">How often do you drink alcohol?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">MTRANS</td>\n",
    "    <td class=\"tg-7zrl\">Categorical</td>\n",
    "    <td class=\"tg-7zrl\">Which transportation do you usually use?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7zrl\">NObeyesdad</td>\n",
    "    <td class=\"tg-7zrl\">Categorical</td>\n",
    "    <td class=\"tg-7zrl\">Obesity level</td>\n",
    "  </tr>\n",
    "</tbody></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Load the data set by executing the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/GkDzb7bWrtvGXdPOfk6CIg/Obesity-level-prediction-dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of the target variable to understand the class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "sns.countplot(y='NObeyesdad', data=data)\n",
    "plt.title('Distribution of Obesity Levels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the dataset is fairly balanced and does not require any special attention in terms of biased training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Check for null values, and display a summary of the dataset (use `.info()` and `.describe()` methods).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "# Checking for null values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Dataset summary\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "* Counts of null values for each column (likely zero for this dataset).\n",
    "* Dataset info including column names, data types, and memory usage.\n",
    "* Descriptive statistics for numerical columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "Scale the numerical features to standardize their ranges for better model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing continuous numerical features\n",
    "continuous_columns = data.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[continuous_columns])\n",
    "\n",
    "# Converting to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=scaler.get_feature_names_out(continuous_columns))\n",
    "\n",
    "# Combining with the original dataset\n",
    "scaled_data = pd.concat([data.drop(columns=continuous_columns), scaled_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of data is important to better define the decision boundaries between classes by making sure that the feature variations are in similar scales. The data is now ready to be used for training and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "Convert categorical variables into numerical format using one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying categorical columns\n",
    "categorical_columns = scaled_data.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_columns.remove('NObeyesdad')  # Exclude target column\n",
    "\n",
    "# Applying one-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_features = encoder.fit_transform(scaled_data[categorical_columns])\n",
    "\n",
    "# Converting to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Combining with the original dataset\n",
    "prepped_data = pd.concat([scaled_data.drop(columns=categorical_columns), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will observe that all the categorical variables have now been modified to one-hot encoded features. This increases the overall number of fields to 24. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable\n",
    "prepped_data['NObeyesdad'] = prepped_data['NObeyesdad'].astype('category').cat.codes\n",
    "prepped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the input and target data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing final dataset\n",
    "X = prepped_data.drop('NObeyesdad', axis=1)\n",
    "y = prepped_data['NObeyesdad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data set\n",
    "Split the data into training and testing subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with One-vs-All\n",
    "In the One-vs-All approach:\n",
    "\n",
    "* The algorithm trains a single binary classifier for each class.\n",
    "* Each classifier learns to distinguish a single class from all the others combined.\n",
    "* If there are k classes, k classifiers are trained.\n",
    "* During prediction, the algorithm evaluates all classifiers on each input, and selects the class with the highest confidence score as the predicted class.\n",
    "\n",
    "#### Advantages:\n",
    "* Simpler and more efficient in terms of the number of classifiers (k)\n",
    "* Easier to implement for algorithms that naturally provide confidence scores (e.g., logistic regression, SVM).\n",
    "\n",
    "#### Disadvantages:\n",
    "* Classifiers may struggle with class imbalance since each binary classifier must distinguish between one class and the rest.\n",
    "* Requires the classifier to perform well even with highly imbalanced datasets, as the \"all\" group typically contains more samples than the \"one\" class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model using the One-vs-All strategy and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training logistic regression model using One-vs-All (default)\n",
    "model_ova = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
    "model_ova.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now evaluate the accuracy of the trained model as a measure of its performance on unseen testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_ova = model_ova.predict(X_test)\n",
    "\n",
    "# Evaluation metrics for OvA\n",
    "print(\"One-vs-All (OvA) Strategy\")\n",
    "print(f\"Accuracy: {np.round(100*accuracy_score(y_test, y_pred_ova),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with OvO\n",
    "\n",
    "In the One-vs-One approach:\n",
    "* The algorithm trains a binary classifier for every pair of classes in the dataset.\n",
    "* If there are k classes, this results in $k(k-1)/2$ classifiers.\n",
    "* Each classifier is trained to distinguish between two specific classes, ignoring the rest.\n",
    "* During prediction, all classifiers are used, and a \"voting\" mechanism decides the final class by selecting the class that wins the majority of pairwise comparisons.\n",
    "\n",
    "#### Advantages:\n",
    "* Suitable for algorithms that are computationally expensive to train on many samples because each binary classifier deals with a smaller dataset (only samples from two classes).\n",
    "* Can be more accurate in some cases since classifiers focus on distinguishing between two specific classes at a time.\n",
    "\n",
    "#### Disadvantages:\n",
    "* Computationally expensive for datasets with a large number of classes due to the large number of classifiers required.\n",
    "* May lead to ambiguous predictions if voting results in a tie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model using the One-vs-One (OvO) strategy and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training logistic regression model using One-vs-One\n",
    "model_ovo = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
    "model_ovo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy of the trained model as a measure of its performance on unseen testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_ovo = model_ovo.predict(X_test)\n",
    "\n",
    "# Evaluation metrics for OvO\n",
    "print(\"One-vs-One (OvO) Strategy\")\n",
    "print(f\"Accuracy: {np.round(100*accuracy_score(y_test, y_pred_ovo),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Experiment with different test sizes in the train_test_split method (e.g., 0.1, 0.3) and observe the impact on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "for test_size in [0.1, 0.3]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "    model_ova.fit(X_train, y_train)\n",
    "    y_pred = model_ova.predict(X_test)\n",
    "    print(f\"Test Size: {test_size}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Plot a bar chart of feature importance using the coefficients from the One vs All logistic regression model. Also try for the One vs One model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "# Feature importance\n",
    "feature_importance = np.mean(np.abs(model_ova.coef_), axis=0)\n",
    "plt.barh(X.columns, feature_importance)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "For One vs One, change `model_ova` to `model_ovo` in the code above.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Write a function `obesity_risk_pipeline` to automate the entire pipeline: <br>\n",
    "<ol>\n",
    "<li> Loading and preprocessing the data </li>\n",
    "<li> Training the model </li>\n",
    "<li> Evaluating the model </li>\n",
    "</ol>\n",
    "The function should accept the file path and test set size as the input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your function here and then execute this cell\n",
    "def obesity_risk_pipeline(data_path, test_size):\n",
    "    ...\n",
    "\n",
    "\n",
    "obesity_risk_pipeline(file_path, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "def obesity_risk_pipeline(data_path, test_size=0.2):\n",
    "    # Load data\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Standardizing continuous numerical features\n",
    "    continuous_columns = data.select_dtypes(include=['float64']).columns.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(data[continuous_columns])\n",
    "    \n",
    "    # Converting to a DataFrame\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=scaler.get_feature_names_out(continuous_columns))\n",
    "    \n",
    "    # Combining with the original dataset\n",
    "    scaled_data = pd.concat([data.drop(columns=continuous_columns), scaled_df], axis=1)\n",
    "\n",
    "    # Identifying categorical columns\n",
    "    categorical_columns = scaled_data.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_columns.remove('NObeyesdad')  # Exclude target column\n",
    "    \n",
    "    # Applying one-hot encoding\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    encoded_features = encoder.fit_transform(scaled_data[categorical_columns])\n",
    "    \n",
    "    # Converting to a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    \n",
    "    # Combining with the original dataset\n",
    "    prepped_data = pd.concat([scaled_data.drop(columns=categorical_columns), encoded_df], axis=1)\n",
    "    \n",
    "    # Encoding the target variable\n",
    "    prepped_data['NObeyesdad'] = prepped_data['NObeyesdad'].astype('category').cat.codes\n",
    "\n",
    "    # Preparing final dataset\n",
    "    X = prepped_data.drop('NObeyesdad', axis=1)\n",
    "    y = prepped_data['NObeyesdad']\n",
    "   \n",
    "    # Splitting data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "    \n",
    "    # Training and evaluation\n",
    "    model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You're ready to move on to your next lesson!\n",
    " \n",
    "## Author\n",
    " \n",
    "<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abishek Gagneja</a>\n",
    " \n",
    " \n",
    " ### Other Contributors\n",
    " \n",
    "<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "## Changelog\n",
    " \n",
    "| Date | Version | Changed by | Change Description |\n",
    "\n",
    "|:------------|:------|:------------------|:---------------------------------------|\n",
    "\n",
    "| 2024-11-05 | 1.0  Abhishek Gagnejan    | Fresh version created |\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "688f362e61a4f318ed825850470c0aa63ee726743053a10eeeebf1bfc0b625ae"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
